{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab838f6833b5dfdb",
   "metadata": {},
   "source": [
    "# Integrating Embeddings with Queries in an Information Retrieval System\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this exercise, we will learn how to integrate embeddings with a query to enhance an Information Retrieval (IR) system. We will use both static and contextual embeddings to generate representations of queries and documents, compute their similarities, and rank the documents based on relevance to the query.\n",
    "\n",
    "---\n",
    "\n",
    "## Stages Covered\n",
    "\n",
    "1. **Introduction to Pre-trained Transformer Models**\n",
    "   - Load and use BERT for contextual embeddings.\n",
    "   - Load and use Word2Vec for static embeddings.\n",
    "\n",
    "2. **Generating Text Embeddings**\n",
    "   - Generate embeddings for queries and documents using BERT.\n",
    "   - Generate embeddings for queries and documents using Word2Vec.\n",
    "\n",
    "3. **Computing Similarity Between Embeddings**\n",
    "   - Compute cosine similarity between query and document embeddings.\n",
    "   - Rank documents based on similarity scores.\n",
    "\n",
    "4. **Integrating Embeddings with Queries**\n",
    "   - Practical implementation of embedding-based retrieval for a given text corpus.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- TensorFlow\n",
    "- Hugging Face's Transformers library\n",
    "- Gensim library\n",
    "- Scikit-learn library\n",
    "- A text corpus in the `../data` folder\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Follow the steps below to integrate embeddings with a query and enhance your IR system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96332a5cf057238",
   "metadata": {},
   "source": [
    "Step 0: Verify requirements:\n",
    "\n",
    "* tensorflow\n",
    "* transformers\n",
    "* scikit-learn\n",
    "* matplotlib\n",
    "* seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f333a9f10d93ad",
   "metadata": {},
   "source": [
    "Step 1: Download dataset from Kaggle\n",
    "\n",
    "URL: https://www.kaggle.com/datasets/zynicide/wine-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab6504ab1e044a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:05:14.438938Z",
     "start_time": "2024-06-26T21:05:12.777794Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'data/winemag-data_first150k.csv' with the actual path to your downloaded CSV file\n",
    "wine_df = pd.read_csv('data/winemag-data_first150k.csv')\n",
    "\n",
    "# Now you can work with the DataFrame as usual\n",
    "print(wine_df.head())\n",
    "corpus = wine_df['description']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef79a9c697bc01",
   "metadata": {},
   "source": [
    "Step 2: Load a Pre-trained Transformer Model\n",
    "\n",
    "Use the BERT model for generating contextual embeddings and Word2Vec for static embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68840411ddabd35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:06:28.907164Z",
     "start_time": "2024-06-26T21:05:21.570520Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gensim.downloader as api\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ce2c32618191",
   "metadata": {},
   "source": [
    "Step 3: Generate Text Embeddings\n",
    "\n",
    "Static Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53983765626d5a85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T21:07:16.011459Z",
     "start_time": "2024-06-26T21:07:15.967338Z"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Funci贸n para generar los embeddings de Word2Vec\n",
    "def generate_word2vec_embedding(text):\n",
    "    tokens = text.lower().split()\n",
    "    word_vectors = [word2vec_model[word] for word in tokens if word in word2vec_model]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Funci贸n para generar embeddings para una lista de textos\n",
    "def generate_word2vec_embeddings(texts):\n",
    "    embeddings = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_text = {executor.submit(generate_word2vec_embedding, text): text for text in texts}\n",
    "        for future in concurrent.futures.as_completed(future_to_text):\n",
    "            embeddings.append(future.result())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Ejemplo de uso\n",
    "word2vec_embeddings = generate_word2vec_embeddings(corpus)\n",
    "print(\"Word2Vec Embeddings:\", word2vec_embeddings)\n",
    "print(\"Word2Vec Shape:\", word2vec_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eee6a2cae63f9",
   "metadata": {},
   "source": [
    "Contextual Embeddings with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddc992f933fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T02:43:37.685104Z",
     "start_time": "2024-06-27T02:43:33.304379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funci贸n para generar los embeddings de BERT\n",
    "def generate_bert_embedding(text):\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n",
    "    # Generate BERT embeddings\n",
    "    outputs = model(**inputs)\n",
    "    # Return the first token's embedding\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Funci贸n para generar embeddings para una lista de textos\n",
    "def generate_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_text = {executor.submit(generate_bert_embedding, text): text for text in texts}\n",
    "        for future in concurrent.futures.as_completed(future_to_text):\n",
    "            embeddings.append(future.result())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Ejemplo de uso\n",
    "bert_embeddings = generate_bert_embeddings(corpus)\n",
    "print(\"BERT Embeddings:\", bert_embeddings)\n",
    "print(\"BERT Shape:\", bert_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f905d2f5fe852",
   "metadata": {},
   "source": [
    "Step 4: Compute Similarity Between Embeddings\n",
    "\n",
    "Use the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1a78ceff1d324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T02:46:20.622551Z",
     "start_time": "2024-06-27T02:46:20.606815Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cosine similarity between Word2Vec embeddings\n",
    "word2vec_embeddings = word2vec_embeddings.squeeze()\n",
    "word2vec_similarity = cosine_similarity(word2vec_embeddings)\n",
    "print(\"Word2Vec Cosine Similarity:\\n\", word2vec_similarity)\n",
    "\n",
    "# Cosine similarity between BERT embeddings\n",
    "bert_embeddings = bert_embeddings.squeeze()\n",
    "bert_similarity = cosine_similarity(bert_embeddings)\n",
    "print(\"BERT Cosine Similarity:\\n\", bert_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46c4c975cb5c7b",
   "metadata": {},
   "source": [
    "Step 5: Compare Contextual and Static Embeddings\n",
    "\n",
    "Analyze and compare the similarity results from both BERT and Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416e3d9334cd21e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:25:28.401064Z",
     "start_time": "2024-06-27T03:25:27.726620Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_similarity_matrix(matrix, title, figsize=(8, 6), annotation=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(matrix, annot=annotation, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_similarity_matrix(word2vec_similarity, \"Word2Vec Cosine Similarity\")\n",
    "plot_similarity_matrix(bert_similarity, \"BERT Cosine Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bd3d58f27a375",
   "metadata": {},
   "source": [
    "Step 6: Applying to Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696a6b93f33889b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T03:27:06.030478Z",
     "start_time": "2024-06-27T03:25:33.511959Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generar embeddings para el corpus\n",
    "corpus_word2vec_embeddings = generate_word2vec_embeddings(corpus)\n",
    "corpus_bert_embeddings = generate_bert_embeddings(corpus)\n",
    "\n",
    "# Asegurarse de que los embeddings tengan la forma correcta\n",
    "corpus_word2vec_embeddings = corpus_word2vec_embeddings.squeeze()\n",
    "corpus_bert_embeddings = corpus_bert_embeddings.squeeze()\n",
    "\n",
    "# Computar la similitud del coseno para el corpus\n",
    "corpus_word2vec_similarity = cosine_similarity(corpus_word2vec_embeddings)\n",
    "corpus_bert_similarity = cosine_similarity(corpus_bert_embeddings)\n",
    "\n",
    "# Mostrar las matrices de similitud\n",
    "plot_similarity_matrix(corpus_word2vec_similarity, \"Corpus Word2Vec Cosine Similarity\", figsize=(16, 12), annotation=False)\n",
    "plot_similarity_matrix(corpus_bert_similarity, \"Corpus BERT Cosine Similarity\", figsize=(16, 12), annotation=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc424507979ba874",
   "metadata": {},
   "source": [
    "Step 7: Generate Embeddings for the Query\n",
    "\n",
    "Generate embeddings for the query using the same model used for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658207660a566a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la consulta\n",
    "query = \"Sample query text for information retrieval\"\n",
    "\n",
    "# Generar embeddings para la consulta usando Word2Vec\n",
    "query_word2vec_embedding = generate_word2vec_embedding(query).reshape(1, -1)\n",
    "print(\"Query Word2Vec Embedding Shape:\", query_word2vec_embedding.shape)\n",
    "\n",
    "# Generar embeddings para la consulta usando BERT\n",
    "query_bert_embedding = generate_bert_embedding(query).numpy().reshape(1, -1)\n",
    "print(\"Query BERT Embedding Shape:\", query_bert_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389cb8f527843221",
   "metadata": {},
   "source": [
    "Step 8: Compute Similarity Between Query and Documents\n",
    "\n",
    "Compute the similarity between the query embedding and each document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5991a0aeababffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la similitud del coseno entre la consulta y los documentos (Word2Vec)\n",
    "query_word2vec_similarity = cosine_similarity(query_word2vec_embedding, corpus_word2vec_embeddings)\n",
    "print(\"Query-Document Word2Vec Cosine Similarity:\", query_word2vec_similarity)\n",
    "\n",
    "# Calcular la similitud del coseno entre la consulta y los documentos (BERT)\n",
    "query_bert_similarity = cosine_similarity(query_bert_embedding, corpus_bert_embeddings)\n",
    "print(\"Query-Document BERT Cosine Similarity:\", query_bert_similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139fea5d43934c96",
   "metadata": {},
   "source": [
    "Step 9: Retrieve and Rank Documents Based on Similarity Scores\n",
    "\n",
    "Retrieve and rank the documents based on their similarity scores to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a6b13bbb20d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar y clasificar documentos basados en la similitud (Word2Vec)\n",
    "word2vec_sorted_indices = np.argsort(-query_word2vec_similarity[0])\n",
    "sorted_word2vec_docs = [corpus[i] for i in word2vec_sorted_indices]\n",
    "\n",
    "# Recuperar y clasificar documentos basados en la similitud (BERT)\n",
    "bert_sorted_indices = np.argsort(-query_bert_similarity[0])\n",
    "sorted_bert_docs = [corpus[i] for i in bert_sorted_indices]\n",
    "\n",
    "# Mostrar los documentos clasificados\n",
    "print(\"Top documents based on Word2Vec similarity:\")\n",
    "for doc in sorted_word2vec_docs[:5]:\n",
    "    print(doc)\n",
    "\n",
    "print(\"\\nTop documents based on BERT similarity:\")\n",
    "for doc in sorted_bert_docs[:5]:\n",
    "    print(doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
